# -*- coding: utf-8 -*-
"""03 ComputerVision.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ugLgRP_5z-TXAUF2ZipMOtCYgAWA4dZk
"""

import torch
from torch import nn

import torchvision
from torchvision import datasets
from torchvision import transforms
from torchvision.transforms import ToTensor

import matplotlib.pyplot as plt

# Getting the dataset

# Setup training data
train_data = datasets.FashionMNIST(
    root = "data", # Where to download data to
    train = True, # Get the training datasets, instead of the testing
    download = True, # Download it locally
    transform = ToTensor(), # Convert the images into tensors
    target_transform = None # We don't want to transform the target labels
)

# Setup testing data
test_data = datasets.FashionMNIST(
    root = "data", # Where to download data to
    train = False, # Get the training datasets, instead of the testing
    download = True, # Download it locally
    transform = ToTensor(), # Convert the images into tensors
    target_transform = None # We don't want to transform the target labels
)

class_names = train_data.classes

# Visualizing data

image, label = train_data[0]
print(f" Image shape: {image.shape}")

plt.imshow(image.squeeze(), cmap = 'gray') # We need to provide only height and width, only ([28, 28])
plt.axis(False);
plt.title(label);

# Prepare DataLoader, currently we have a DataSet
# To do so, we want to turn our data into batches or mini-batches:
  # 1. More computationally efficient
  # 2. It gives our NN more chances to update its gradients for epoch, so more chances to learn and get better.

from torch.utils.data import DataLoader

BATCH_SIZE = 32

train_dataloader = DataLoader(dataset = train_data,
                              batch_size = BATCH_SIZE,
                              shuffle = True) # To mix the data

test_dataloader =  DataLoader(dataset = test_data,
                          batch_size = BATCH_SIZE,
                          shuffle = False) # Better for evalution

# Check out what's inside the dataloader batch

train_features_batch, train_labels_batch = next(iter(train_dataloader))
print(train_features_batch.shape, train_labels_batch.shape)

test_features_batch, test_labels_batch = next(iter(test_dataloader))
print(test_features_batch.shape, test_labels_batch.shape)

# Make preidction and get results

def eval_model(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, accuracy_fn):
  """ Returns a dictionray containing the rsults of model predicting on data_loader """
  loss, acc = 0, 0
  model.eval()

  with torch.inference_mode():
    for x, y in data_loader:
      # Make predictions
      y_pred = model(x)

      # Accumulate the loss per batch
      loss += loss_fn(y_pred, y)
      acc += accuracy_fn(y, y_pred.argmax(dim = 1))

    # Scale loss to find the average loss per batch
    loss /= len(data_loader)
    acc /= len(data_loader)

  return {"model_name": model.__class__.__name__, # Only works when model is created as class
          "model_loss": loss.item(),
          "model_acc": acc}

# Building a non-linear model

device = "cuda" if torch.cuda.is_available() else "cpu"
device

# Training Functions

def train_step(model: torch.nn.Module,
               data_loader: torch.utils.data.DataLoader,
               loss_fn: torch.nn.Module,
               optimizer: torch.optim.Optimizer,
               accuracy_fn,
               device: torch.device = device):

  """ Performs a training model trying to learn on data_loader"""

  train_loss, train_acc = 0, 0 # Resets the loss and acc to 0

  # Put model on training mode
  model.train()

  # Add a loop to loop through the training batches
  for batch, (x, y) in enumerate(data_loader):
    # Put data on target device
    x, y = x.to(device), y.to(device)

    model.train()

    # Do the forward pass (raw logits)
    y_pred = model(x)

    # Calculate the loss and acc
    loss = loss_fn(y_pred, y)
    train_loss += loss # Accumulate train loss
    train_acc += accuracy_fn(y, y_pred.argmax(dim = 1))

    # Optimizer zero grad
    optimizer.zero_grad()

    # Loss backward
    loss.backward()

    # Optimizer step, step, step
    optimizer.step()

  # Divide total train loss by length of train dataloader
  train_loss /= len(data_loader)
  train_acc /= len(data_loader)

  # Print out what's happening
  print(f"\n Train Loss: {train_loss:.5f}, Train Accuracy: {train_acc:.4f}")

"""------------------------------------------------"""


def test_step(model: torch.nn.Module,
              data_loader: torch.utils.data.DataLoader,
              loss_fn: torch.nn.Module,
              accuracy_fn,
              device: torch.device):

  test_loss, test_acc = 0, 0  # Resets the loss and acc to 0

  # Put model in evaluation mode
  model.eval()

  # Add a loop to loop through the testing batches
  with torch.inference_mode():
      for x, y in data_loader:
          # Put data on target device
          x, y = x.to(device), y.to(device)

          # Do the forward pass (raw logits)
          y_pred = model(x)

          # Calculate the loss and acc
          test_loss += loss_fn(y_pred, y).item()  # Extract scalar for accumulation
          test_acc += accuracy_fn(y, y_pred.argmax(dim=1))  # Ensure accuracy_fn returns scalar

  # Divide total test loss and accuracy by length of test dataloader
  test_loss /= len(data_loader)
  test_acc /= len(data_loader)

  # Print out what's happening
  print(f"\n Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.4f}")

# Create a CNN
import numpy as np

class CNNV2(nn.Module):

  def __init__(self, in_shape: int, hidden_units: int, out_shape: int):
    super().__init__()

    # A combination of different layers: Convolutinal Layer, ReLU, and Pooling
    self.conv_block_1 = nn.Sequential(
        nn.Conv2d(in_channels = in_shape,
                  out_channels = hidden_units,
                  kernel_size = 3,
                  stride = 1,
                  padding = 1),
        nn.ReLU(),
        nn.Conv2d(in_channels = hidden_units,
                  out_channels = hidden_units,
                  kernel_size = 3,
                  stride = 1,
                  padding = 1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size = 2)
    )

    self.conv_block_2 = nn.Sequential(
        nn.Conv2d(in_channels = hidden_units,
                  out_channels = hidden_units,
                  kernel_size = 3,
                  stride = 1,
                  padding = 1),
        nn.ReLU(),
        nn.Conv2d(in_channels = hidden_units,
                  out_channels = hidden_units,
                  kernel_size = 3,
                  stride = 1,
                  padding = 1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size = 2)
    )

    self.classifier = nn.Sequential(
        nn.Flatten(),
        nn.Linear(in_features = hidden_units*7*7, # Because now we have a tensor large: ([batches, hidden_units * width/2/2 * height/2/2])
                  out_features = out_shape)
    )

  def forward(self, x, visualize=False):
    if visualize:
      self.visualize_feature_maps(x, "Input Image")

    x = self.conv_block_1(x)
    if visualize:
      self.visualize_feature_maps(x, "After Conv Block 1")

    x = self.conv_block_2(x)
    if visualize:
      self.visualize_feature_maps(x, "After Conv Block 2")

    x = self.classifier(x)
    if visualize:
      self.visualize_probabilities(x, class_names)

    return x

  @staticmethod
  def visualize_feature_maps(x, title):
    """Visualizes feature maps with improved handling of layout."""
    num_filters = x.shape[1]
    rows, cols = int(np.ceil(num_filters / 8)), min(8, num_filters)  # Dynamic grid size
    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2.5, rows * 2.5))
    fig.suptitle(title, fontsize=18, fontweight="bold", y=1.02)

    # Flatten axes for easy iteration
    axes = axes.flat if hasattr(axes, "flat") else [axes]
    im = None  # Initialize to store the last plotted image

    for i, ax in enumerate(axes):
      if i < num_filters:
        feature_map = x[0, i].detach().cpu().numpy()
        im = ax.imshow(feature_map, cmap="gray", aspect="auto")
        ax.set_title(f"Filter {i + 1}", fontsize=10)
        ax.axis("off")
      else:
        ax.axis("off")

    # Add a single color bar if any filter exists
    if im:
      cbar = fig.colorbar(im, ax=axes[-1], orientation="vertical", fraction=0.02, pad=0.04)
      cbar.set_label("Activation Strength", fontsize=12)

    # Adjust layout manually to fit elements without overlapping
    fig.subplots_adjust(wspace=0.4, hspace=0.6)
    plt.show()

  @staticmethod
  def visualize_probabilities(x, class_names=None):
      """Visualizes the output probabilities as a clean, modern bar chart."""
      probabilities = nn.Softmax(dim=1)(x).detach().cpu().numpy().flatten()
      class_labels = class_names if class_names else [f"Class {i}" for i in range(len(probabilities))]

      # Highlight the top prediction
      top_class = np.argmax(probabilities)
      colors = ['#e0e0e0'] * len(probabilities)  # Base color
      colors[top_class] = '#66b2ff'  # Highlight top prediction

      # Create the bar chart
      plt.figure(figsize=(12, 6))
      bars = plt.bar(
          class_labels,
          probabilities,
          color=colors,
          edgecolor='black',
          linewidth=0.6,
          width=0.6,
          alpha = 0.7
      )

      # Add probability values on top of each bar
      for i, prob in enumerate(probabilities):
          plt.text(
              i,
              prob + 0.02,
              f"{prob:.2f}",
              ha="center",
              va="bottom",
              fontsize=12,
              color="black"
          )

      # Customize the aesthetics
      plt.gca().spines['top'].set_visible(False)
      plt.gca().spines['right'].set_visible(False)
      plt.gca().spines['left'].set_visible(False)
      plt.grid(axis='y', linestyle='--', alpha=0.7)
      plt.xticks(fontsize=12, rotation=45, ha='right')
      plt.yticks(fontsize=12)
      plt.ylim(0, 1.1)
      plt.xlabel("Class", fontsize=14, labelpad=10, fontweight="bold")
      plt.ylabel("Probability", fontsize=14, labelpad=10, fontweight="bold")
      plt.title("Predicted Class Probabilities", fontsize=16, fontweight="bold", pad=15)

      # Add legend for the highlighted top prediction
      plt.legend(
          [bars[top_class]],
          ["Top Prediction"],
          loc='upper right',
          fontsize=12,
          frameon=False
      )

      plt.tight_layout()
      plt.show()

model_2 = CNNV2(in_shape = 1, # 1 Because we have only one color channel
                hidden_units = 10,
                out_shape = len(class_names)).to(device)

model_2(image.unsqueeze(0).to(device), visualize = True)

# Loss Function and Optimizer

loss_fn = nn.CrossEntropyLoss() # Since it's a multiclass dataset

optimizer = torch.optim.Adam(params = model_2.parameters(),
                             lr = 0.01)

def accuracy_fn(y_true, y_pred):
  correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal
  acc = (correct / len(y_pred)) * 100
  return acc

from timeit import default_timer as timer

def print_train_time(start: float, end: float, device: torch.device = None):
    """Prints difference between start and end time.

    Args:
        start (float): Start time of computation (preferred in timeit format).
        end (float): End time of computation.
        device ([type], optional): Device that compute is running on. Defaults to None.

    Returns:
        float: time between start and end in seconds (higher is longer).
    """
    total_time = end - start
    print(f"Train time on {device}: {total_time:.3f} seconds")
    return total_time

train_time_start_model_2 = timer()

# Train and test model
epochs = 10
for epoch in range(epochs):
    print(f"Epoch: {epoch}\n---------")
    train_step(data_loader = train_dataloader,
        model = model_2,
        loss_fn = loss_fn,
        optimizer = optimizer,
        accuracy_fn = accuracy_fn,
        device = device
    )
    test_step(data_loader = test_dataloader,
        model = model_2,
        loss_fn = loss_fn,
        accuracy_fn = accuracy_fn,
        device = device
    )

train_time_end_model_2 = timer()
total_train_time_model_2 = print_train_time(start = train_time_start_model_2,
                                           end = train_time_end_model_2,
                                           device = device)

model_2(image.unsqueeze(0).to(device), visualize = True)

# Making a confusion matrix
  # 1. Make predictions with our trained model on the test dataset
  # 2. Make a confusion matrix with `torchmetrics.ConfusionMatrix`
  # 3. Plot the confusion matrix using `mixtend.plotting.plot_confusion_matrix()`

y_preds = []

model_2.eval()
with torch.inference_mode():
  for x, y in test_dataloader:
    # Send the data and targets to target device
    x, y = x.to(device), y.to(device)

    # Do the forward pass
    y_logits = model_2(x)

    # Turn predictions from logits -> prediction porbabilites -> prediction labels
    y_pred = torch.softmax(y_logits.squeeze(), dim = 0).argmax(dim = 1)

    # Put predictions on CPU for evaluation
    y_preds.append(y_pred.cpu())

  # List of predictions into a tensor
  y_pred_tensor = torch.cat(y_preds)

# Importing torchmetrics and mlxtend

try:
  import torchmetrics, mlxtend
  print(f"mlxtend version: {mlxtend.__version__}")
  print(f"torchmetrics version: {torchmetrics.__version__}")
  assert int(mlxtend.__version__.split(".")[1]) >= 19, "mlxtend version should be 0.19.0 or higher"
except:
  !pip install -q torchmetrics mlxtend
  import torchmetrics, mlxtend
  print(f"mlxtend version: {mlxtend.__version__}")
  print(f"torchmetrics version: {torchmetrics.__version__}")

from torchmetrics import ConfusionMatrix
from mlxtend.plotting import plot_confusion_matrix

# 2. Setup confuison instance and compare predictions to target
confmat = ConfusionMatrix(task = "multiclass", num_classes = len(class_names))
confmat_tensor = confmat(preds = y_pred_tensor,
                         target = test_data.targets)

# 3. Plot the confusion matrix
fig, ax = plot_confusion_matrix(
    conf_mat = confmat_tensor.numpy(),
    class_names = class_names,
    figsize = (10, 7)
)

