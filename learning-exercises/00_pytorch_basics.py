# -*- coding: utf-8 -*-
"""00 PyTorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wU2pUsz1Qwc5_XwOnoKXu9ZS_Euwe-kf
"""

import torch
from torch import nn # nn contains all of PyTorch building's blocks for neural networks
import matplotlib.pyplot as plt

# Create known parameters
weight = 0.7
bias = 0.3

# Create the dataset
start = 0
end = 1
step = 0.02
x = torch.arange(start, end, step).unsqueeze(dim=1) # The inputs
y = weight * x + bias # The desired outputs, with our known parameters

# Create a train/test split
train_split = int(0.8 * len(x))
x_train, y_train = x[:train_split], y[:train_split] # From the start to the `train split` number (in this case 40)
x_test, y_test = x[train_split:], y[train_split:] # From the `train split` number (in this case 40) to the end

def plot_predictions(train_data= x_train,
                     train_labels = y_train,
                     test_data = x_test,
                     test_labels = y_test,
                     predictions = None):
  """
  Plots trainig data, test data and compares prediction
  """

  plt.figure(figsize = (10, 7))

  # Plot trainig data in blue
  plt.scatter(train_data, train_labels, c = "b", s = 4, label = "Training data") # c = color (b = blue, g = green, r = red)

  # Plot test data in green
  plt.scatter(test_data, test_labels, c = "g", s = 4, label = "Testing data")

  # Are there predictions
  if predictions is not None:
    # Plot the predictions if they exist
    plt.scatter(test_data, predictions, c = "r", s = 4, label = "Predictions")

  # Show the legend
  plt.legend(prop={"size": 9})

plot_predictions()

# Create linear regression model class
class LinearRegressionModel(nn.Module): # It is a child of nn.Module. Basically every class we will create will be a child of this module
  def __init__(self):
    super().__init__() # For initializing the attributes of nn.Module

    # Initialize model parameters. By suing nn.Parameter, these tensors will be added to the attribute .parameters of this class, in order to be easiy called
    self.weight = nn.Parameter(torch.randn(1,
        requires_grad = True,
        dtype = torch.float
    ))

    self.bias = nn.Parameter(torch.randn(1,
        requires_grad = True,
        dtype = torch.float
    ))

  # Forward method to define the computation in the model
  def forward(self, x: torch.Tensor) -> torch.Tensor: # <- 'x' is the input data
    return self.weight * x + self.bias # this is the output formula, but this time with random parameters

# Checking the contents of our model
# Create a random seed, in order to get the same `random` weights and biases each time
torch.manual_seed(42)

# Creating an instance of the model
model_0 = LinearRegressionModel()

# Checking the parameters
list(model_0.parameters()) # We can use `.parameters` since weight and bias are defined as `nn.Parameter`

# List named parameters
model_0.state_dict() # Give the dictionary of the model

# Calculating the current loss
loss_fn = nn.L1Loss()

# Setting an optimizer (stochastic gradient descent)
optimizer = torch.optim.SGD(params = model_0.parameters(), # the parameters we want to optimize
                            lr = 0.005 #learning rate#
                            )

# Initialize a list to track different value
losses = []
epochs_count = []
test_losses = []

# Building a trainig and testing loop

# Number of epochs
epochs = 400

# Training loop, passing the data through the model for a number of epochs
for epoch in range(epochs):

    # Put the model in training mode
    model_0.train()

    # 1. Forward pass on *train data* using the `forward()` method
    y_pred = model_0(x_train)

    # 2. Calculate the current loss
    loss = loss_fn(y_pred, y_train)
    losses.append(loss.item())  # Save the loss value as a float

    # 3. Zero the previous gradient in order for them to not accumulate and to be recalculated each time
    optimizer.zero_grad()

    # 4. Perform backpropagation on the loss function
    loss.backward()

    # 5. Step the optimizer, basically gradient descent
    optimizer.step()

    epochs_count.append(epoch)

    # Print loss for the current epoch
    # print(f"Epoch {epoch+1}/{epochs} | Loss: {loss.item()}")

# 6. Testing
def testing():
    model_0.eval() # Turns off settings in the model not needed for evaluation/testing
    with torch.inference_mode(): # Turns off griedn tracking and a couple more settings
      # 1. Do the forward pass
      test_pred = model_0(x_test)

      # 2. Calculate the test loss
      test_loss = loss_fn(test_pred, y_test)

      test_losses.append(test_loss.item())

      print(f"Epoch {len(epochs_count)}/{len(epochs_count)} | Test Loss: {loss.item()}")

      # Plotting the loss change over epochs
      plt.figure(figsize=(10, 6))
      plt.plot(range(1, len(epochs_count) + 1), losses, label="Training Loss")
      plt.xlabel("Epochs")
      plt.ylabel("Loss")
      plt.legend()
      plt.show()

      # Plotting the prediction in red
      plot_predictions(predictions = test_pred)

testing()

# Saving our PyTorch model
from pathlib import Path

# 1. Create models directory
MODEL_PATH = Path("models")
MODEL_PATH.mkdir(parents=True, exist_ok=True)

# 2. Create model save path
MODEL_NAME = "0_model.pth" # `.pt` and `.pth` are standard for saving PyTorch object
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

# 3. Save the model state dict, instead of the overall model, convenient for memory
print(f"Saving model to: {MODEL_SAVE_PATH}")
torch.save(obj = model_0.state_dict(), # The object that we want to save
           f = MODEL_SAVE_PATH) # The file directory of this object

# Loading our PyTorch model state_dict

# To load in a saved state_dict we have to insantiate a new instance of our model class
loaded_model_0 = LinearRegressionModel()

# Load the saved state_dict of model_0
loaded_model_0.load_state_dict(torch.load(f = MODEL_SAVE_PATH))

with torch.inference_mode():
  loaded_model_predictions = loaded_model_0(x_test)

plot_predictions(predictions = loaded_model_predictions)

