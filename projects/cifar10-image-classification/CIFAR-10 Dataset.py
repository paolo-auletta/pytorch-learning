# -*- coding: utf-8 -*-
"""Computer Vision 02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AgslnmxJro2SJituTgckDa1rg6yPAV4b
"""

import matplotlib.pyplot as plt
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
device

# Transformations for CIFAR-10 dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]
])

# Load CIFAR-10 Dataset
train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)
test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

class CNN(nn.Module):

  def __init__(self, in_channels: int, out_channels: int):
    super().__init__()

    self.Layer_1 = nn.Sequential(
        nn.Conv2d(in_channels = in_channels,
                  out_channels = 32,
                  kernel_size = 3,
                  stride = 1),
        nn.ReLU(),
        nn.BatchNorm2d(32)
    )

    self.Layer_2 = nn.Sequential(
        nn.Conv2d(in_channels = 32,
                  out_channels = 64,
                  kernel_size = 3,
                  stride = 1),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size = 2,
                     stride = 2),
        nn.BatchNorm2d(64)
    )

    self.Classifier = nn.Sequential(
        nn.Flatten(),
        nn.Linear(in_features = 64 * 14 * 14, # Since it has been flattened
                  out_features = 512),
        nn.ReLU(),
        nn.Dropout(p=0.5),  # Drop 50% of neurons
        nn.Linear(in_features = 512,
                  out_features = out_channels)
    )

  def forward(self, x):
    #print(x.shape)
    x = self.Layer_1(x)
    #print(x.shape)
    x = self.Layer_2(x)
    #print(x.shape)
    x = self.Classifier(x)
    #print(x.shape)

    return x

model_0 = CNN(in_channels = 3,
              out_channels = 10).to(device)

model_0

test = torch.randn((1, 3, 32, 32)).to(device)
model_0(test)

# Setup Loss function, Optimizer and Accuracy metric

loss_fn = nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(params = model_0.parameters(),
                             lr = 0.001,
                             weight_decay=1e-4)

def accuracy_fn(y_pred, y_true):
    correct = torch.eq(y_pred, y_true).sum().item()
    return (correct / y_true.size(0)) * 100

# Store metrics for visualization
train_loss_list, train_acc_list = [], []
test_loss_list, test_acc_list = [], []

total_epochs = []

# Loading our PyTorch model state_dict
from pathlib import Path

MODEL_PATH = Path("models")
MODEL_NAME = "model_0.pth" # `.pt` and `.pth` are standard for saving PyTorch object
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

# Load the saved state_dict of model_0
model_0.load_state_dict(torch.load(f = MODEL_SAVE_PATH))

# Training Loop
epochs = 10

for epoch in range(epochs):
    print(f"Epoch {epoch+1}/{epochs} \n --------")
    total_epochs.append(epoch)

    model_0.train()  # Set the model to training mode
    train_loss, train_acc = 0, 0

    # Add tqdm for the training loop
    train_loader_tqdm = tqdm(enumerate(train_loader), total=len(train_loader), desc="Training")
    for batch, (x, y) in train_loader_tqdm:
        x, y = x.to(device), y.to(device)

        # Forward pass
        y_logits = model_0(x)
        y_pred = y_logits.argmax(dim=1)

        # Loss
        loss = loss_fn(y_logits, y)
        train_loss += loss.item()

        # Accuracy
        train_acc += accuracy_fn(y_pred, y)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Update progress bar with batch loss
        train_loader_tqdm.set_postfix({"Batch Loss": loss.item()})

    train_loss /= len(train_loader)
    train_acc /= len(train_loader)
    train_loss_list.append(train_loss)
    train_acc_list.append(train_acc)
    print(f"Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc:.2f}%")

    model_0.eval()  # Set the model to evaluation mode
    test_loss, test_acc = 0, 0

    # Add tqdm for the test loop
    test_loader_tqdm = tqdm(test_loader, desc="Evaluating", total=len(test_loader))
    with torch.no_grad():
        for x, y in test_loader_tqdm:
            x, y = x.to(device), y.to(device)

            # Forward pass
            test_logits = model_0(x)
            test_pred = test_logits.argmax(dim=1)

            # Loss
            loss = loss_fn(test_logits, y)
            test_loss += loss.item()

            # Accuracy
            test_acc += accuracy_fn(test_pred, y)

    test_loss /= len(test_loader)
    test_acc /= len(test_loader)
    test_loss_list.append(test_loss)
    test_acc_list.append(test_acc)
    print(f"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%")

# Visualization of training and testing metrics
epochs_range = range(1, epochs + 1)

plt.figure(figsize=(12, 6))

# Plot Loss
plt.subplot(1, 2, 1)
plt.plot(epochs_range, train_loss_list, label='Train Loss')
plt.plot(epochs_range, test_loss_list, label='Test Loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot Accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs_range, train_acc_list, label='Train Accuracy')
plt.plot(epochs_range, test_acc_list, label='Test Accuracy')
plt.title('Accuracy Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy (%)')
plt.legend()

plt.tight_layout()
plt.show()

# 2. Plotting the loss and accuracy over epochs in a style similar to the provided example
plt.figure(figsize=(14, 8))

# Plot loss over epochs
plt.subplot(1, 2, 1)
plt.plot(range(len(total_epochs)), train_loss_list, color='lightblue', linestyle='-', linewidth=1, alpha=0.8, label="Train Loss")
plt.plot(range(len(total_epochs)), test_loss_list, color='steelblue', linestyle='-', linewidth=1, alpha=0.8, label="Test Loss")
plt.title("Loss Over Epochs", fontsize=16)
plt.xlabel("Epochs", fontsize=14)
plt.ylabel("Loss", fontsize=14)
plt.legend()
plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.3)

# Plot accuracy over epochs
plt.subplot(1, 2, 2)
plt.plot(range(len(total_epochs)), train_acc_list, color='lightcoral', linestyle='-', linewidth=1, alpha=0.8, label="Train Accuracy")
plt.plot(range(len(total_epochs)), test_acc_list, color='tomato', linestyle='-', linewidth=1, alpha=0.8, label="Test Accuracy")
plt.title("Accuracy Over Epochs", fontsize=16)
plt.xlabel("Epochs", fontsize=14)
plt.ylabel("Accuracy (%)", fontsize=14)
plt.legend()
plt.grid(True, which='both', linestyle='--', linewidth=0.5, alpha=0.3)

# Add tight layout for better spacing
plt.tight_layout()

# Show the plots
plt.show()

# Load CIFAR-10 Dataset
test_dataset_2 = datasets.CIFAR10(root='./data', train=False, transform=transforms.ToTensor(), download=True)

test_loader_2 = DataLoader(test_dataset_2, batch_size=64, shuffle=True)

# Fetch the first batch of test data
test_iter = iter(test_loader_2)
images, labels = next(test_iter)

images, labels = images.to(device), labels.to(device)


model_0.eval()

# Make predictions
with torch.no_grad():
    logits = model_0(images)
    predictions = logits.argmax(dim=1)

# Visualize predictions
fig, axes = plt.subplots(1, 10, figsize=(16, 4))
classes = train_dataset.classes  # CIFAR-10 class names

for i in range(10):  # Display the first 8 images
    ax = axes[i]
    img = images[i].cpu().permute(1, 2, 0)  # Convert to HWC format

    ax.imshow(img.numpy())
    ax.set_title(f"Pred: {classes[predictions[i]]}\nTrue: {classes[labels[i]]}")
    ax.axis("off")

plt.tight_layout()
plt.show()

